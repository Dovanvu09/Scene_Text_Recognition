{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face = 'Arial'  size = '6'> 1. Import Libraries </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.92  Python-3.12.4 torch-2.3.0 CPU (AMD Ryzen 5 5500U with Radeon Graphics)\n",
      "Setup complete  (12 CPUs, 19.4 GB RAM, 26.9/100.1 GB disk)\n"
     ]
    }
   ],
   "source": [
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import timm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face = 'Arial' size = '6'> 2. Load Model </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face = 'Arial' size = '4'> 2.1 Loade YOLO </face>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "text_det_model_path = ' '\n",
    "yolo = YOLO(text_det_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face = 'Arial' size = '4'> 2.2 Load CRNN </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = '0123456789abcdefghijklmnopqrstuvwxyz-'\n",
    "vocab_size = len(chars)\n",
    "char_to_idx = {char : idx +1 for idx, char in  enumerate(sorted(chars))}\n",
    "idx_to_char = {idx : char for char, idx in char_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (4270271084.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    def __init__()\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size,\n",
    "            hidden_size,\n",
    "            n_layers,\n",
    "            dropout = 0.2,\n",
    "            unfreeze_layers = 3\n",
    "    ):\n",
    "        super(CRNN, self).__init__()\n",
    "\n",
    "        # initialize pretrained model resnet101\n",
    "        backbone = timm.create_model(\n",
    "            'resnet101',\n",
    "            in_chans = 1,\n",
    "            pretrained = True\n",
    "        )\n",
    "\n",
    "        # remove the original pretrained classification class\n",
    "        modules = list(backbone.children())[:2]\n",
    "        # add the adaptiveAvgPool2d class\n",
    "        modules.append(nn.AdaptiveAvgPool2d((1, None)))\n",
    "        self.backbone = nn.Sequential(*modules)\n",
    "\n",
    "        # unfreeze some final layer of the pretrained model\n",
    "        for parameter in self.backbone[-unfreeze_layers : ].parameters():\n",
    "            parameter.requires_grad = True\n",
    "        \n",
    "        # layer map from CNN features maps to LSTM\n",
    "        self.mapSeq = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            1024, hidden_size,\n",
    "            n_layers, bidirectional = True, batch_first = True,\n",
    "            droppout = dropout if n_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2 )\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2 , vocab_size),\n",
    "            nn.LogSoftmax(dim = 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x) # shape (bs , channels, height, width)\n",
    "        x = x.permute(0, 3, 1, 2) # shape ( bs, w , c, h)\n",
    "        x = x.view(x.size(0), x.size(1), -1)\n",
    "        x = self.mapSeq(x)\n",
    "        x,_ = self.lstm(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.out(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        return x\n",
    "    \n",
    "hidden_size = 256\n",
    "n_layers = 2\n",
    "dropout_prob = 0.3\n",
    "unfreeze_layers=3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_path = 'models/ocr_crnn_resnet_best.pt'\n",
    "\n",
    "crnn_model = CRNN(\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=hidden_size,\n",
    "    n_layers=n_layers,\n",
    "    dropout=dropout_prob,\n",
    "    unfreeze_layers=unfreeze_layers\n",
    ").to(device)\n",
    "crnn_model.load_state_dict(torch.load(model_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face = 'Arial' size = '6'> 3. Inference</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(encoded_sequences, idx_to_char, blank_char = '-'):\n",
    "    \"\"\"\n",
    "    Decode encoded_sequences to string\n",
    "    parameters:\n",
    "       encoded_sequences (list) : The lists tensor label\n",
    "       idx_to_char (dict) : mapping ID -> classname\n",
    "       blank_char(str) : '-'\n",
    "\n",
    "    Return:\n",
    "          decoded_sequences (list) : The List of decoded_labels.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Declare empty list to contains result decoded\n",
    "    decoded_sequences = []\n",
    "\n",
    "    for seq in encoded_sequences:\n",
    "        \n",
    "        decoded_label = []\n",
    "\n",
    "        for idx, token in enumerate(seq) :\n",
    "            if token != 0: \n",
    "                char = idx_to_char[token.item()]\n",
    "                if char != blank_char:\n",
    "                    decoded_label.append(char)\n",
    "        \n",
    "        decoded_sequences.append(''.join(decoded_label))\n",
    "    \n",
    "    return decoded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_detection(img_path, text_det_model):\n",
    "    \"\"\"\n",
    "    Locate (bbox) the text in the image\n",
    "\n",
    "    Parameters: \n",
    "         img_path (str) : path to file image\n",
    "         text_det_model (YOLO) : Model YOLO text detection\n",
    "    \n",
    "    Returns:\n",
    "         tuple: Includes identified components(bboxes, classes, names, confs)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Perform detection according to YOLO\n",
    "    text_det_results = text_det_model(img_path, verbose = False)[0]\n",
    "\n",
    "    bboxes = text_det_results.boxes.xyxy.tolist()\n",
    "    # get classes, confidence scores\n",
    "    classes = text_det_results.boxes.cls.tolist()\n",
    "    names = text_det_results.names\n",
    "    confs = text_det_results.boxes.conf.tolist()\n",
    "\n",
    "    return bboxes, classes, names, confs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_recognition(img, data_transforms, text_reg_model, idx_to_char, device):\n",
    "    \"\"\"\n",
    "    Recognition text in image\n",
    "    Parameters:\n",
    "        img(PIL.image) : image Object\n",
    "        data_transforms (transforms.Compose) : Preprocessing.\n",
    "        text_reg_model (CRNN) : Model CRNN text recognition.\n",
    "        idx_to_char (dict) :  mapping ID -> classname\n",
    "    \n",
    "        Returns :\n",
    "            text(str) : output text\n",
    "        \"\"\"\n",
    "    \n",
    "    transformed_image = data_transforms(img)\n",
    "    transformed_image = transformed_image.unsqueeze(0).to(device)\n",
    "    text_reg_model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = text_reg_model(transformed_image).detach().cpu()\n",
    "    text = decode(logits.permute(1, 0, 2).argmax(2), idx_to_char)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections(img, detections):\n",
    "    \"\"\"\n",
    "    Visualize result Scene Text Recognition (STR) \n",
    "\n",
    "    Parameters :\n",
    "        img (PIL.Image) : image Object\n",
    "        detections (list) : The lists contains result STR on the image \n",
    "    \"\"\"\n",
    "    plt.figure(figsize = (12, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    for bbox, detected_class, confidence, transcribed_text in detections : \n",
    "        x1, y1, x2, y2 = bbox\n",
    "        plt.gca().add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill = False, edgecolor = 'red', linewidth = 2))\n",
    "        plt.text(\n",
    "            x1, y1-10, f\"{detected_class} ({confidence:.2f}) : {transcribed_text}\",\n",
    "            fontsize = 9, bbox = dict(facecolor = 'red', alpha = 0.5)\n",
    "        )\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train' : transforms.Compose([\n",
    "        transforms.Resize((100, 420)),\n",
    "        transforms.ColorJitter(brightness = 0.5, constrast = 0.5, saturation = 0.5),\n",
    "        transforms.Graysscale(num_output_channels = 1 ),\n",
    "        transforms.GaussianBlur(3),\n",
    "        transforms.RandomAffine(degress = 1, sehar = 1),\n",
    "        transforms.RandomPerspective(distortion_scale = 0.2, p = 0.3, interpolation = 3),\n",
    "        transforms.RandomRotation(degrees = 2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]),\n",
    "    'val'  : transforms.Compose([\n",
    "        transforms.Resize((100,420)),\n",
    "        transforms.Grayscale(num_outout_channels = 1),\n",
    "        transforms.ToTnesor(),\n",
    "        transforms.Normalize((0.5,), (0.5)),\n",
    "    ])\n",
    "}\n",
    "def predict(img_path, data_transforms, text_det_model, text_reg_model,\n",
    "            idx_to_char, device, visualize = True):\n",
    "    \"\"\"\n",
    "    Scene Text Recognition with any image\n",
    "    parameters:\n",
    "       img_path (str) : path to image\n",
    "       data_transfroms (trainsforms.compose) : function preprocessing image\n",
    "       text_det_model ( YOLO) : model YOLO text detection\n",
    "       text_reg_model (CRNN) : model CRNN text recognition\n",
    "       idx_to_char ( dict) : mapping idx -> classname\n",
    "       device(str) : 'cpu' or 'gpu'\n",
    "       visualize (bool) : visualization result STR\n",
    "    Returns : \n",
    "    predictions (list): The list results STR on image\n",
    "    \"\"\"\n",
    "    bboxes, classes, names, confs = text_detection(img_path, text_det_model)\n",
    "\n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for bbox, cls, conf in zip(bboxes, classes, confs):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        confidence = conf\n",
    "        detected_class = cls\n",
    "        name = name[int(cls)] \n",
    "\n",
    "        cropped_image = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "        transcribed_text = text_recognition(\n",
    "            cropped_image,\n",
    "            data_transforms,\n",
    "            text_reg_model,\n",
    "            idx_to_char,\n",
    "            device\n",
    "        )\n",
    "\n",
    "        predictions.append(( bbox, name, confidence, transcribed_text))\n",
    "\n",
    "    if visualize:\n",
    "        visualize_detections(img, predictions)\n",
    "    \n",
    "    return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = ''\n",
    "inf_transforms = data_transforms['val']\n",
    "for img_filename in os.listdir(img_dir):\n",
    "    img_path = os.path.join(img_dir, img_filename)\n",
    "    predictions = predict(\n",
    "        img_path, \n",
    "        data_transforms=inf_transforms, \n",
    "        text_det_model=yolo, \n",
    "        text_reg_model=crnn_model, \n",
    "        idx_to_char=idx_to_char,\n",
    "        device=device,\n",
    "        visualize=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
